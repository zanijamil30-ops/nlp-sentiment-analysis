# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRkR8U7KUenYC8Gl3PZjqYYL60kSzIMf

Import Required Libraries
"""

# ===== Step 1: Import Required Libraries =====

# Data handling
import pandas as pd
import numpy as np

# Text cleaning
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# NLP (optional but recommended)
import spacy

# Machine Learning (TF-IDF + Model + Metrics)
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Other
import joblib

# ===== Download NLTK & spaCy resources =====
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# spaCy English model
nlp = spacy.load("en_core_web_sm")

print("Libraries imported successfully!")

"""Load Dataset"""

# ===== Step 2: Load Dataset from Local Machine (Colab) =====

from google.colab import files
import pandas as pd

# Upload your CSV file (a file picker window will open)
uploaded = files.upload()

# Automatically read the uploaded file
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)

# Basic checks
print("Dataset shape:", df.shape)
print("\nColumn names:", df.columns.tolist())

# Show first 5 rows
df.head()

"""Exploratory Data Analysis (EDA)"""

# ===== Step 3: EDA =====

# 3.1 Check missing values
print("Missing values per column:\n")
print(df.isnull().sum())

# 3.2 Class distribution
print("\nClass distribution:\n")
label_counts = df['label'].value_counts()
print(label_counts)

# 3.3 Show 3 examples from each class
print("\nExample posts from each class:")
for lbl in label_counts.index:
    print(f"\n--- {lbl} ---")
    examples = df[df['label'] == lbl]['text'].head(3).tolist()
    for ex in examples:
        print("-", ex)

# 3.4 Plot label distribution
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(6,4))
sns.countplot(x='label', data=df, order=label_counts.index, palette='viridis')
plt.title("Label Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Text Preprocessing"""

# ===== FIX missing NLTK resources =====
import nltk

nltk.download('punkt')        # tokenizer
nltk.download('punkt_tab')    # NEW tokenizer table (fixes your error)
nltk.download('stopwords')
nltk.download('wordnet')

# ===== Step 4: Text Preprocessing =====

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = str(text).lower()                                                    # Lowercase
    text = re.sub(r"http\S+|www\.\S+", "", text)                                # Remove URLs
    text = re.sub(r"<.*?>", " ", text)                                          # Remove HTML tags
    text = text.encode('ascii', 'ignore').decode('ascii')                        # Remove emojis / non-ASCII
    text = re.sub(r"[^a-z\s]", " ", text)                                       # Remove punctuation/numbers
    text = re.sub(r"\s+", " ", text).strip()                                    # Remove extra spaces

    # Tokenize + Lemmatize using NLTK
    tokens = nltk.word_tokenize(text)
    cleaned_tokens = []
    for token in tokens:
        if token not in stop_words and len(token) > 1:
            lemma = lemmatizer.lemmatize(token)
            cleaned_tokens.append(lemma)

    return " ".join(cleaned_tokens)

# Apply preprocessing
df['clean_text'] = df['text'].apply(clean_text)

# Show sample
df[['text', 'clean_text', 'label']].head()

df[['clean_text', 'label']].sample(10)

df['clean_length'] = df['clean_text'].apply(lambda x: len(x.split()))
df[['clean_text', 'clean_length']].head()

for i in range(10):
    print("Original:", df.loc[i, 'text'])
    print("Cleaned :", df.loc[i, 'clean_text'])
    print("Label   :", df.loc[i, 'label'])
    print("-" * 60)

"""Split Dataset"""

# ===== Step 5: Train-Test Split =====

X = df['clean_text']    # features
y = df['label']         # target labels

from sklearn.model_selection import train_test_split

# stratify=y keeps the same Positive/Negative/Neutral ratio in both sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.20,
    random_state=42,
    stratify=y
)

print("Training size:", len(X_train))
print("Testing size :", len(X_test))

print("\nTraining label distribution:")
print(y_train.value_counts())

print("\nTesting label distribution:")
print(y_test.value_counts())

"""Convert Text to Numbers"""

# ===== Step 6 — TF-IDF Vectorization =====

from sklearn.feature_extraction.text import TfidfVectorizer

# configure vectorizer
tfidf = TfidfVectorizer(
    max_features=10000,      # keep top 10k features
    ngram_range=(1,2),       # unigrams + bigrams
    stop_words=None,         # we already removed stopwords in preprocessing
    lowercase=False,         # text already lowercased in preprocessing
)

# Fit on TRAIN and transform both train & test
X_train_vec = tfidf.fit_transform(X_train)
X_test_vec  = tfidf.transform(X_test)

print("TF-IDF vector shapes:")
print("  X_train_vec:", X_train_vec.shape)
print("  X_test_vec :", X_test_vec.shape)

# Optional: inspect a few feature names
try:
    print("\nSample features:", tfidf.get_feature_names_out()[:20])
except:
    pass

"""Train Machine Learning Model"""

# ===== Step 7: Train Logistic Regression =====

from sklearn.linear_model import LogisticRegression

# Create the model
log_reg = LogisticRegression(
    max_iter=1000,        # ensures convergence
    random_state=42
)

# Train the model on the TF-IDF vectors
log_reg.fit(X_train_vec, y_train)

print("Logistic Regression model trained successfully!")

"""Model Evaluation"""

# ===== Step 8: Model Evaluation (Logistic Regression) =====

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Predict
y_pred = log_reg.predict(X_test_vec)
y_train_pred = log_reg.predict(X_train_vec)

# 2) Train vs Test accuracy (quick overfit check)
train_acc = accuracy_score(y_train, y_train_pred)
test_acc  = accuracy_score(y_test, y_pred)
print(f"Train accuracy: {train_acc:.4f}")
print(f"Test  accuracy: {test_acc:.4f}\n")

# 3) Classification report
print("Classification Report (test):\n")
print(classification_report(y_test, y_pred, digits=4))

# 4) Per-class metrics as a tidy DataFrame
precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, labels=log_reg.classes_, zero_division=0)
metrics_df = pd.DataFrame({
    "label": log_reg.classes_,
    "precision": precision,
    "recall": recall,
    "f1": f1,
    "support": support
}).sort_values("f1", ascending=False).reset_index(drop=True)
print("\nPer-class metrics:")
display(metrics_df)

# 5) Confusion matrix (raw counts)
cm = confusion_matrix(y_test, y_pred, labels=log_reg.classes_)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=log_reg.classes_, yticklabels=log_reg.classes_, cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (counts)")
plt.tight_layout()
plt.show()

# 6) Normalized confusion matrix (proportion per true class)
cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]
plt.figure(figsize=(6,5))
sns.heatmap(cm_norm, annot=True, fmt='.2f', xticklabels=log_reg.classes_, yticklabels=log_reg.classes_, cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (normalized by true class)")
plt.tight_layout()
plt.show()

# 7) Top positive features per class (interpretability)
try:
    feat_names = tfidf.get_feature_names_out()
    coefs = log_reg.coef_  # shape (n_classes, n_features) in sklearn's one-vs-rest
    for i, label in enumerate(log_reg.classes_):
        top_idx = np.argsort(coefs[i])[-15:][::-1]   # top 15 features for this class
        top_feats = feat_names[top_idx]
        print(f"\nTop words for class '{label}':\n", ", ".join(top_feats))
except Exception as e:
    print("Could not show top features (reason):", e)

# 8) Optional: Save metrics to CSV for report
metrics_df.to_csv("logreg_per_class_metrics.csv", index=False)
print("\nSaved per-class metrics to: logreg_per_class_metrics.csv")

"""Visualize Results"""

# ===== Additional Visualizations (no repeats) =====
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score

# 1) ROC curves (one-vs-rest) + AUC (multiclass)
labels = list(log_reg.classes_)
y_test_binarized = label_binarize(y_test, classes=labels)
n_classes = y_test_binarized.shape[1]

# get probability estimates
if hasattr(log_reg, "predict_proba"):
    y_score = log_reg.predict_proba(X_test_vec)
else:
    # fallback: decision function (SVMs) -- scale into [0,1] per class
    y_score = log_reg.decision_function(X_test_vec)
    # if decision_function returns 1d for binary, adjust accordingly

plt.figure(figsize=(8,6))
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f"{labels[i]} (AUC = {roc_auc:.2f})")

plt.plot([0,1], [0,1], 'k--', lw=1)
plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.title("ROC Curves (One-vs-Rest)")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# 2) Precision-Recall curves + average precision per class
plt.figure(figsize=(8,6))
for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(y_test_binarized[:, i], y_score[:, i])
    ap = average_precision_score(y_test_binarized[:, i], y_score[:, i])
    plt.plot(recall, precision, lw=2, label=f"{labels[i]} (AP = {ap:.2f})")

plt.xlabel("Recall"); plt.ylabel("Precision")
plt.title("Precision-Recall Curves (per class)")
plt.legend(loc="upper right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# 3) Top TF-IDF features as bar charts per class (visual)
try:
    feat_names = tfidf.get_feature_names_out()
    coefs = log_reg.coef_          # shape (n_classes, n_features)
    top_k = 12
    plt.figure(figsize=(12, 4 * n_classes))
    for i, label in enumerate(labels):
        top_idx = np.argsort(coefs[i])[-top_k:]
        top_vals = coefs[i][top_idx]
        top_feats = feat_names[top_idx]
        ax = plt.subplot(n_classes, 1, i+1)
        sns.barplot(x=top_vals[::-1], y=top_feats[::-1], palette="viridis", ax=ax)
        ax.set_title(f"Top {top_k} positive TF-IDF features for '{label}'")
        ax.set_xlabel("Coefficient value")
    plt.tight_layout()
    plt.show()
except Exception as e:
    print("Could not plot top features:", e)

# 4) Distribution of cleaned text lengths by class (helps spot length bias)
if 'clean_text' in df.columns:
    df['clean_len'] = df['clean_text'].apply(lambda s: len(str(s).split()))
    plt.figure(figsize=(8,5))
    sns.boxplot(x='label', y='clean_len', data=df)
    plt.title("Distribution of cleaned text length by class")
    plt.xlabel("Label"); plt.ylabel("Number of tokens (clean_text)")
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(8,5))
    sns.histplot(data=df, x='clean_len', hue='label', element='step', stat='density', common_norm=False)
    plt.title("Clean text length density by class")
    plt.xlabel("Number of tokens (clean_text)")
    plt.tight_layout()
    plt.show()
else:
    print("Column 'clean_text' not found — cannot plot length distributions.")

"""Save Your Model"""

# ===== Step 10: Save Model and Vectorizer =====

import joblib

# Save the trained Logistic Regression model
joblib.dump(log_reg, "logistic_regression_model.pkl")

# Save the TF-IDF vectorizer
joblib.dump(tfidf, "tfidf_vectorizer.pkl")

print("Model saved as: logistic_regression_model.pkl")
print("TF-IDF vectorizer saved as: tfidf_vectorizer.pkl")

"""Load model & test on new text"""

# ===== Load Model & Vectorizer =====

import joblib

log_reg = joblib.load("logistic_regression_model.pkl")
tfidf = joblib.load("tfidf_vectorizer.pkl")


# ===== Define a simple predict function =====

def predict_sentiment(text):
    # Clean using the same preprocessing function
    cleaned = clean_text(text)
    vec = tfidf.transform([cleaned])
    pred = log_reg.predict(vec)[0]
    return pred


# ===== Test on new text =====
samples = [
    "I absolutely love this product!",
    "This is the worst thing I ever bought.",
    "It's okay, nothing special."
]

for s in samples:
    print(f"Text: {s}\nPredicted Sentiment: {predict_sentiment(s)}\n")